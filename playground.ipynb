{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "import openml\n",
    "from simple_model import DyadOneHotPairDataset, DyadRankingModel, create_dyads, ConformalPredictor, ConformalRankingPredictor, MCDyadOneHotPairDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/Documents/Research/dyad_ranking/torch_plnet/.venv/lib/python3.11/site-packages/openml/utils.py:461: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dataset = openml.datasets.get_dataset(35)\n",
    "X, y, _, _ = dataset.get_data(\n",
    "    target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "# Automatically identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(\n",
    "    include=[\"object\", \"category\"]\n",
    ").columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "# Preprocessing for numerical data: Impute missing values, then scale\n",
    "numerical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocessing for categorical data: Impute missing values, then one-hot encode\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numerical_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "\n",
    "if not isinstance(X_train, np.ndarray):\n",
    "    X_train = X_train.toarray()\n",
    "if not isinstance(y_train, np.ndarray):\n",
    "    y_train = y_train.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m pred_sets_rf \u001b[38;5;241m=\u001b[39m cp_rf\u001b[38;5;241m.\u001b[39mpredict_set(X_test)\n\u001b[1;32m     29\u001b[0m crp \u001b[38;5;241m=\u001b[39m ConformalRankingPredictor(num_classes\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mcrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cross_isntance_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m pred_sets_rnk \u001b[38;5;241m=\u001b[39m crp\u001b[38;5;241m.\u001b[39mpredict_set(X_test)\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/simple_model.py:523\u001b[0m, in \u001b[0;36mConformalRankingPredictor.fit\u001b[0;34m(self, X, y, random_state, num_epochs, cal_size, use_cross_isntance_data, num_pairs, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m X_train, X_cal, y_train, y_cal \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39mcal_size)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m DyadRankingModel(\n\u001b[1;32m    521\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m y\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim\n\u001b[1;32m    522\u001b[0m )\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cross_instance_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cross_isntance_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# here we usually compute non conformity scores. For the ranker\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# we use the predicted latent skill value\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# y_pred_cal = self.model.predict_proba(X_cal)\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# self.scores = 1 - y_pred_cal[np.arange(len(y_cal)), y_cal]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m cal_dyads \u001b[38;5;241m=\u001b[39m create_dyads(X_cal, y_cal, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/simple_model.py:425\u001b[0m, in \u001b[0;36mDyadRankingModel.fit\u001b[0;34m(self, X, y, num_epochs, learning_rate, num_classes, batch_size, val_frac, random_state, use_cross_instance_dataset, num_pairs, patience, delta)\u001b[0m\n\u001b[1;32m    423\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    424\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/simple_model.py:346\u001b[0m, in \u001b[0;36mDyadRankingModel._fit\u001b[0;34m(self, train_loader, val_loader, learning_rate, num_epochs, random_state, patience, delta)\u001b[0m\n\u001b[1;32m    344\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    345\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 346\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/.venv/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:282\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    274\u001b[0m     (inputs,)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    281\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 282\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/Documents/Research/dyad_ranking/torch_plnet/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:78\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     74\u001b[0m     reg_grad_shape \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_grads_batched \u001b[38;5;28;01melse\u001b[39;00m grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reg_out_shape, reg_grad_shape\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_grads\u001b[39m(\n\u001b[1;32m     79\u001b[0m     outputs: Sequence[torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m     80\u001b[0m     grads: Sequence[_OptionalTensor],\n\u001b[1;32m     81\u001b[0m     is_grads_batched: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m     82\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_OptionalTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m     83\u001b[0m     new_grads: List[_OptionalTensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m out, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, grads):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from simple_model import ClassifierModel, DyadRankingModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# clf = ClassifierModel(input_dim = X_train.shape[1], hidden_dim=16, output_dim=y.max()+1)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "clf = ClassifierModel(input_dim=X_train.shape[1], hidden_dim=16, output_dim=num_classes)\n",
    "\n",
    "cp_net = ConformalPredictor(clf, alpha=0.05)\n",
    "cp_rf = ConformalPredictor(rf, alpha=0.05)\n",
    "\n",
    "cp_net.fit(X_train, y_train, num_epochs=100, random_state=1, patience=8)\n",
    "cp_rf.fit(X_train, y_train)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "if not isinstance(X_test, np.ndarray):\n",
    "    X_test = X_test.toarray()\n",
    "if not isinstance(y_test, np.ndarray):\n",
    "    y_test = y_test.toarray()\n",
    "\n",
    "\n",
    "\n",
    "pred_sets_clf = cp_net.predict_set(X_test)\n",
    "pred_sets_rf = cp_rf.predict_set(X_test)\n",
    "\n",
    "crp = ConformalRankingPredictor(num_classes=y_train.max()+1, alpha= 0.05)\n",
    "crp.fit(X_train,y_train, random_state=1, use_cross_isntance_data=True, num_epochs=100, patience=8)\n",
    "\n",
    "pred_sets_rnk = crp.predict_set(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy clf 0.43243243243243246\n",
      "Accuracy rnk 0.33783783783783783\n",
      "Accuracy rf 0.5405405405405406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "coverage_clf = np.mean([y_test[i] in pred_sets_clf[i] for i in range(len(y_test))])\n",
    "efficiency_clf = np.mean([len(pred_sets_clf[i]) for i in range(len(y_test))])\n",
    "\n",
    "# coverage_rnk = np.mean([y_test[i] in pred_sets_rnk[i] for i in range(len(y_test))])\n",
    "# efficiency_rnk = np.mean([len(pred_sets_rnk[i]) for i in range(len(y_test))])\n",
    "y_test_clf = cp_net.model.predict(X_test)\n",
    "y_test_rnk = crp.model.predict(X_test)\n",
    "y_test_rf = cp_rf.model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy clf {accuracy_score(y_test_clf, y_test)}\")\n",
    "print(f\"Accuracy rnk {accuracy_score(y_test_rnk, y_test)}\")\n",
    "print(f\"Accuracy rf {accuracy_score(y_test_rf, y_test)}\")\n",
    "\n",
    "# print(f\"Coverage clf {coverage_clf} efficiency clf {efficiency_clf}\")\n",
    "# print(f\"Coverage rnk {coverage_rnk} efficiency rnk {efficiency_rnk}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 2, 0, 1, 2, 2, 0, 2, 2,\n",
       "       2, 1, 0, 2, 2, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.sum(X, axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_model import MCDyadOneHotPairDataset, DyadOneHotPairDataset\n",
    "ds = DyadOneHotPairDataset(X, y, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for x in ds:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 1., 0., 1., 0.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [0., 1., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 1., 0., 1., 0.]])\n",
      "tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 1.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 0., 0.]])\n",
      "tensor([[1., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_mc = MCDyadOneHotPairDataset(X, y, num_classes=3, num_pairs=-1)\n",
    "for x in ds_mc:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from simple_model import TabularDataset\n",
    "random_state = 1\n",
    "\n",
    "X,y = make_classification(n_samples=1000, n_classes=5, n_informative=5)\n",
    "dataset = TabularDataset(X, y)\n",
    "gen = torch.Generator().manual_seed(random_state)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [1 - 1/3, 1/3], generator=gen\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "for input, labels in train_loader:\n",
    "    print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from simple_model import TabularDataset\n",
    "from math import ceil, log2\n",
    "random_state = 1\n",
    "num_classes=5\n",
    "\n",
    "X,y = make_classification(n_samples=1000, n_classes=num_classes, n_informative=5)\n",
    "num_pairs = ceil(len(X_train) * log2(len(X_train)) * num_classes)\n",
    "dataset = MCDyadOneHotPairDataset(X, y,num_classes=num_classes,num_pairs=num_pairs, random_state=random_state)\n",
    "gen = torch.Generator().manual_seed(random_state)\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [1 - 1/3, 1/3], generator=gen\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for x in train_loader:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(crp.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
