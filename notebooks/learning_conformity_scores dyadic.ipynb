{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from mapie.classification import MapieClassifier\n",
    "from util.ranking_datasets import DyadOneHotPairDataset\n",
    "from models.ranking_models import DyadRankingModel, SortLayer\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.datasets import make_classification\n",
    "from mapie.conformity_scores import LACConformityScore, APSConformityScore, TopKConformityScore\n",
    "import torch\n",
    "\n",
    "class MultinomialSyntheticDataGenerator(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, random_state=None):\n",
    "        \"\"\"\n",
    "        A custom estimator for generating synthetic data using multinomial logistic regression,\n",
    "        with the feature distribution inferred from the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_samples (int): Number of synthetic samples to generate.\n",
    "        - random_state (int): Seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits a multinomial logistic regression model to the data and estimates the feature distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        - y (ndarray): Target labels of shape (n_samples,).\n",
    "        \n",
    "        Returns:\n",
    "        - self: The fitted instance.\n",
    "        \"\"\"\n",
    "        # Store mean and covariance of features\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.feature_mean_ = np.mean(X, axis=0)\n",
    "        self.feature_cov_ = np.cov(X, rowvar=False)\n",
    "        \n",
    "        # Fit a logistic regression model\n",
    "        self.model_ = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=self.random_state)\n",
    "        self.model_.fit(X, y)\n",
    "        \n",
    "        # Store the number of classes and features\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for the given feature matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        \n",
    "        Returns:\n",
    "        - probabilities (ndarray): Predicted probabilities of shape (n_samples, n_classes).\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"model_\")\n",
    "        return self.model_.predict_proba(X)\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for the given feature matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "        \n",
    "        Returns:\n",
    "        - probabilities (ndarray): Predicted probabilities of shape (n_samples, n_classes).\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"model_\")\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"\n",
    "        Generates synthetic data and labels based on the learned model and feature distribution.\n",
    "        \n",
    "        Returns:\n",
    "        - X_synthetic (ndarray): Generated feature matrix of shape (n_samples, n_features).\n",
    "        - y_synthetic (ndarray): Generated labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"model_\", \"feature_mean_\", \"feature_cov_\"])\n",
    "        \n",
    "        # Generate synthetic features based on the inferred distribution\n",
    "        X_synthetic = np.random.multivariate_normal(self.feature_mean_, self.feature_cov_, n)\n",
    "        \n",
    "        # Compute class probabilities\n",
    "        P_Y_given_X = self.predict_proba(X_synthetic)\n",
    "        \n",
    "        # Sample synthetic labels\n",
    "        y_synthetic = np.array([np.random.choice(self.n_classes_, p=probs) for probs in P_Y_given_X])\n",
    "        \n",
    "        return X_synthetic, y_synthetic\n",
    "\n",
    "\n",
    "    def generate_instances(self, n):\n",
    "        \"\"\"\n",
    "        Generates synthetic data and labels based on the learned model and feature distribution.\n",
    "        \n",
    "        Returns:\n",
    "        - X_synthetic (ndarray): Generated feature matrix of shape (n_samples, n_features).\n",
    "        - y_synthetic (ndarray): Generated labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"model_\", \"feature_mean_\", \"feature_cov_\"])\n",
    "        \n",
    "        # Generate synthetic features based on the inferred distribution\n",
    "        \n",
    "        X = np.random.multivariate_normal(self.feature_mean_, self.feature_cov_, n)\n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OracleAnnotator:\n",
    "    def __init__(self,mapie_clf, generator):\n",
    "        self.mapie_clf = mapie_clf\n",
    "        self.classes_ = mapie_clf.classes_\n",
    "        self.generator = generator\n",
    "\n",
    "    def generate_pairs_in_instance(self, n):\n",
    "        \"\"\"\n",
    "        Generates synthetic data and labels based on the learned model and feature distribution.\n",
    "        \n",
    "        Returns:\n",
    "        - X_synthetic (ndarray): Generated feature matrix of shape (n_samples, n_features).\n",
    "        - y_synthetic (ndarray): Generated labels of shape (n_samples,).\n",
    "        \"\"\"        \n",
    "        # Generate synthetic features based on the inferred distribution\n",
    "        X = self.generator.generate_instances(n)\n",
    "        X = np.repeat(X, repeats=2, axis=0)\n",
    "\n",
    "        y = np.hstack([np.random.choice(self.classes_, size=2, replace=False) for _ in range(n)])\n",
    "\n",
    "        conformities = self.get_conformity(X,y)\n",
    "\n",
    "        X_rs = X.reshape(n,2,self.generator.n_features_)\n",
    "        y_rs = y.reshape(n,2)\n",
    "        conformities_n_rs = - conformities.reshape(n,2)\n",
    "        sort_idx = conformities_n_rs.argsort(axis=1)\n",
    "        X_rs[sort_idx]\n",
    "        y_rs[sort_idx,:]\n",
    "        X_pairs = np.take_along_axis(X_rs, sort_idx[:, :, np.newaxis], axis=1)\n",
    "        y_pairs = np.expand_dims(np.take_along_axis(y_rs, sort_idx, axis=1),axis=-1)\n",
    "        return X_pairs, y_pairs\n",
    "\n",
    "\n",
    "    def generate_pairs_cross_instance(self, n):\n",
    "        \"\"\"\n",
    "        Generates synthetic data and labels based on the learned model and feature distribution.\n",
    "        \n",
    "        Returns:\n",
    "        - X_synthetic (ndarray): Generated feature matrix of shape (n_samples, n_features).\n",
    "        - y_synthetic (ndarray): Generated labels of shape (n_samples,).\n",
    "        \"\"\"        \n",
    "        # Generate synthetic features based on the inferred distribution\n",
    "        \n",
    "        X = self.generator.generate_instances(2*n)\n",
    "        y = np.random.choice(self.classes_, size=2*n, replace=True)\n",
    "        conformities = self.get_conformity(X,y)\n",
    "\n",
    "        X_rs = X.reshape(n,2,self.generator.n_features_)\n",
    "        y_rs = y.reshape(n,2)\n",
    "        conformities_n_rs = - conformities.reshape(n,2)\n",
    "        sort_idx = conformities_n_rs.argsort(axis=1)\n",
    "        X_rs[sort_idx]\n",
    "        y_rs[sort_idx,:]\n",
    "        X_pairs = np.take_along_axis(X_rs, sort_idx[:, :, np.newaxis], axis=1)\n",
    "        y_pairs = np.expand_dims(np.take_along_axis(y_rs, sort_idx, axis=1),axis=-1)\n",
    "\n",
    "        return X_pairs, y_pairs\n",
    "    \n",
    "    def create_pairs_for_classification_data(self, X):\n",
    "        \"\"\"\n",
    "        Generates synthetic data and labels based on the learned model and feature distribution.\n",
    "        \n",
    "        Returns:\n",
    "        - X_synthetic (ndarray): Generated feature matrix of shape (n_samples, n_features).\n",
    "        - y_synthetic (ndarray): Generated labels of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        # Generate synthetic features based on the inferred distribution\n",
    "        \n",
    "        X = self.generator.generate_instances(2*n)\n",
    "        y = np.random.choice(self.classes_, size=2*n, replace=True)\n",
    "        conformities = self.get_conformity(X,y)\n",
    "\n",
    "        X_rs = X.reshape(n,2,self.generator.n_features_)\n",
    "        y_rs = y.reshape(n,2)\n",
    "        conformities_n_rs = - conformities.reshape(n,2)\n",
    "        sort_idx = conformities_n_rs.argsort(axis=1)\n",
    "        X_rs[sort_idx]\n",
    "        y_rs[sort_idx,:]\n",
    "        X_pairs = np.take_along_axis(X_rs, sort_idx[:, :, np.newaxis], axis=1)\n",
    "        y_pairs = np.expand_dims(np.take_along_axis(y_rs, sort_idx, axis=1),axis=-1)\n",
    "\n",
    "        return X_pairs, y_pairs\n",
    "\n",
    "    # we assume y is already label encoded\n",
    "    def get_conformity(self, X, y):\n",
    "        y_pred_proba = self.mapie_clf.estimator.predict_proba(X)\n",
    "        scores = self.mapie_clf.conformity_score_function_.get_conformity_scores(\n",
    "                        y, y_pred_proba, y_enc=y\n",
    "                    )\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from venv import create\n",
    "# from util.ranking_datasets import DyadOneHotPairDataset\n",
    "# from mapie.classification import MapieClassifier\n",
    "# from mapie.conformity_scores.sets import APSConformityScore, LACConformityScore, NaiveConformityScore, TopKConformityScore\n",
    "# from torch.utils.data.dataloader import DataLoader\n",
    "# from sklearn.datasets import make_classification\n",
    "# from scipy.stats import kendalltau\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def create_dyads(X,y, n_classes):\n",
    "#     y_1h = np.eye(n_classes)[y.reshape(-1)].reshape(*y.shape, n_classes)\n",
    "#     dyads = np.concatenate((X, y_1h.squeeze()), axis=1)\n",
    "#     return dyads\n",
    "\n",
    "# X_seed, y_seed = make_classification(n_samples=1000, n_features=3, n_classes=3, n_informative=3, n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42)\n",
    "# conformity_score = APSConformityScore()\n",
    "# generator = MultinomialSyntheticDataGenerator(random_state=42)\n",
    "# generator.fit(X_seed, y_seed)\n",
    "# X_cal, y_cal = generator.generate(n=100)\n",
    "# mapie_clf = MapieClassifier(estimator=generator, cv=\"prefit\", conformity_score=conformity_score)\n",
    "# # create mapie classifier for conformity scores\n",
    "# mapie_clf.fit(X_cal, y_cal)\n",
    "# # create \n",
    "# oracle_annotator = OracleAnnotator(mapie_clf, generator)\n",
    "\n",
    "# # generate all possible pairs for a couple of instances\n",
    "\n",
    "# def create_training_data(n_instances):\n",
    "#     n_classes = len(generator.classes_)\n",
    "#     X_train = generator.generate_instances(n_instances).repeat(n_classes, axis=0)\n",
    "#     y_train = np.tile(generator.classes_, n_instances)\n",
    "#     conformities = oracle_annotator.get_conformity(X_train,y_train)\n",
    "#     sort_idx = (-conformities).argsort(axis=0).flatten()\n",
    "\n",
    "#     X_sorted = X_train[sort_idx]\n",
    "#     y_sorted = y_train[sort_idx]\n",
    "\n",
    "#     X_pairs = np.array([(X_sorted[i], X_sorted[j]) for i in range(len(X_sorted)) for j in range(i + 1, len(X_sorted))])\n",
    "#     y_pairs = np.array([(y_sorted[i], y_sorted[j]) for i in range(len(y_sorted)) for j in range(i + 1, len(y_sorted))])\n",
    "#     y_pairs = np.expand_dims(y_pairs, axis=-1)\n",
    "#     y_pairs_1h = np.eye(n_classes)[y_pairs.reshape(-1)].reshape(*y_pairs.shape, n_classes)\n",
    "#     dyads = np.concatenate((X_pairs, y_pairs_1h.squeeze()), axis=2)\n",
    "#     ds_1h = DyadOneHotPairDataset()\n",
    "#     ds_1h.create_from_numpy_dyad_pairs(dyads)\n",
    "#     return ds_1h, X_train, y_train\n",
    "\n",
    "# from models.ranking_models import DyadRankingModel, SortLayer\n",
    "# import torch\n",
    "\n",
    "# model = DyadRankingModel(input_dim=6,hidden_dims=[6,6,6],activations=[torch.nn.Sigmoid(), SortLayer(), torch.nn.Identity()])\n",
    "\n",
    "\n",
    "# train_data, X_train, y_train = create_training_data(100)\n",
    "# val_data, X_val, y_val = create_training_data(50)\n",
    "\n",
    "# train_loader = DataLoader(train_data, 64)\n",
    "# val_loader = DataLoader(val_data, 64)\n",
    "\n",
    "\n",
    "# model._fit(train_loader,val_loader=val_loader, num_epochs=200, patience=1000, learning_rate=0.01, verbose=True)\n",
    "\n",
    "# def create_dyads(X,y, n_classes):\n",
    "#     y_1h = np.eye(n_classes)[y.reshape(-1)].reshape(*y.shape, n_classes)\n",
    "#     dyads = np.concatenate((X, y_1h.squeeze()), axis=1)\n",
    "#     return dyads\n",
    "\n",
    "# X_test, y_test = generator.generate(10)\n",
    "\n",
    "# conformities = oracle_annotator.get_conformity(X_test, y_test)\n",
    "\n",
    "# dyads_test= create_dyads(X_test, y_test, 3)\n",
    "# dyads_tensor = torch.tensor(dyads_test, dtype=torch.float32)\n",
    "# skills = model(dyads_tensor).detach().cpu().numpy()\n",
    "# print(\"out of sample\", kendalltau(skills, conformities))\n",
    "\n",
    "# X_test, y_test = X_train, y_train\n",
    "\n",
    "# conformities = oracle_annotator.get_conformity(X_test, y_test)\n",
    "\n",
    "# dyads_test= create_dyads(X_test, y_test, 3)\n",
    "# dyads_tensor = torch.tensor(dyads_test, dtype=torch.float32)\n",
    "# skills = model(dyads_tensor).detach().cpu().numpy()\n",
    "# print(\"in of sample\", kendalltau(skills, conformities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_oracle_experiment(conformity_score, num_instances_to_check, generator, X_cal, y_cal):\n",
    "    tau_corrs = []\n",
    "    # Generate a small dataset\n",
    "\n",
    "    mapie_clf = MapieClassifier(estimator=generator, cv=\"prefit\", conformity_score=conformity_score)\n",
    "    # create mapie classifier for conformity scores\n",
    "    mapie_clf.fit(X_cal, y_cal)\n",
    "    # create \n",
    "    oracle_annotator = OracleAnnotator(mapie_clf, generator)\n",
    "    models = []\n",
    "\n",
    "    for num_instances in num_instances_to_check:\n",
    "\n",
    "        X_train = generator.generate_instances(num_instances).repeat(3, axis=0)\n",
    "        y_train = np.tile(generator.classes_, num_instances)\n",
    "        conformities = oracle_annotator.get_conformity(X_train,y_train)\n",
    "        sort_idx = (-conformities).argsort(axis=0).flatten()\n",
    "\n",
    "        X_sorted = X_train[sort_idx]\n",
    "        y_sorted = y_train[sort_idx]\n",
    "\n",
    "        X_pairs = np.array([(X_sorted[i], X_sorted[j]) for i in range(len(X_sorted)) for j in range(i + 1, len(X_sorted))])\n",
    "        y_pairs = np.array([(y_sorted[i], y_sorted[j]) for i in range(len(y_sorted)) for j in range(i + 1, len(y_sorted))])\n",
    "\n",
    "        y_pairs_1h = np.eye(3)[y_pairs.reshape(-1)].reshape(*y_pairs.shape, 3)\n",
    "        dyads = np.concatenate((X_pairs, y_pairs_1h.squeeze()), axis=2)\n",
    "        ds_1h = DyadOneHotPairDataset()\n",
    "        ds_1h.create_from_numpy_dyad_pairs(dyads)\n",
    "        pair_loader = DataLoader(ds_1h, batch_size=64)\n",
    "        model = DyadRankingModel(input_dim=6,hidden_dims=[6,6,6],activations=[torch.nn.Sigmoid(), SortLayer(), torch.nn.ReLU()])\n",
    "\n",
    "        model.num_classes = generator.n_classes_\n",
    "        # model.to(\"cuda\")\n",
    "        device = next(model.parameters()).device\n",
    "        print(f\"Model is on: {device}\")\n",
    "        model._fit(pair_loader, val_loader=pair_loader, num_epochs=250, learning_rate=0.001, patience=100, verbose=True)\n",
    "\n",
    "        # generate data from data generating process and check whether the learned non-conformity relation sorts them correctly\n",
    "        X_test = generator.generate_instances(100).repeat(3, axis=0)\n",
    "        y_test = np.tile(generator.classes_, 100)        \n",
    "        skills_from_model = np.take_along_axis(model.predict_class_skills(X_test), y_test[:,np.newaxis], axis=1)\n",
    "        conformity_scores = oracle_annotator.get_conformity(X_test, y_test)\n",
    "        tau_corr, p_value = kendalltau(skills_from_model, conformity_scores)\n",
    "        tau_corrs.append(tau_corr)\n",
    "        models.append(models)\n",
    "        torch.cuda.empty_cache()\n",
    "    return tau_corrs, skills_from_model, conformity_scores, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/plnet/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n",
      "Epoch 1/250\n",
      "  Train Loss: 0.0993\n",
      "  Val Loss: 0.0993\n",
      "Epoch 2/250\n",
      "  Train Loss: 0.0993\n",
      "  Val Loss: 0.0992\n",
      "Epoch 3/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 4/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 5/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 6/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 7/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 8/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 9/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 10/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 11/250\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0991\n",
      "Epoch 12/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 13/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 14/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 15/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 16/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 17/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 18/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 19/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 20/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 21/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 22/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 23/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0990\n",
      "Epoch 24/250\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0990\n",
      "Epoch 25/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 26/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 27/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 28/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 29/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 30/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 31/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 32/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 33/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 34/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 35/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 36/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 37/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 38/250\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0989\n",
      "Epoch 39/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 40/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 41/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 42/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 43/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 44/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 45/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 46/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 47/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 48/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 49/250\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0988\n",
      "Epoch 50/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 51/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 52/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 53/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 54/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 55/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 56/250\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0987\n",
      "Epoch 57/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 58/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 59/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 60/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 61/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 62/250\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0986\n",
      "Epoch 63/250\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 64/250\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 65/250\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 66/250\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0985\n",
      "Epoch 67/250\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 68/250\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 69/250\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 70/250\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0984\n",
      "Epoch 71/250\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 72/250\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 73/250\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0983\n",
      "Epoch 74/250\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0983\n",
      "Epoch 75/250\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0983\n",
      "Epoch 76/250\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0982\n",
      "Epoch 77/250\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0982\n",
      "Epoch 78/250\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0982\n",
      "Epoch 79/250\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0981\n",
      "Epoch 80/250\n",
      "  Train Loss: 0.0981\n",
      "  Val Loss: 0.0981\n",
      "Epoch 81/250\n",
      "  Train Loss: 0.0981\n",
      "  Val Loss: 0.0980\n",
      "Epoch 82/250\n",
      "  Train Loss: 0.0980\n",
      "  Val Loss: 0.0980\n",
      "Epoch 83/250\n",
      "  Train Loss: 0.0980\n",
      "  Val Loss: 0.0979\n",
      "Epoch 84/250\n",
      "  Train Loss: 0.0980\n",
      "  Val Loss: 0.0979\n",
      "Epoch 85/250\n",
      "  Train Loss: 0.0979\n",
      "  Val Loss: 0.0977\n",
      "Epoch 86/250\n",
      "  Train Loss: 0.0975\n",
      "  Val Loss: 0.0973\n",
      "Epoch 87/250\n",
      "  Train Loss: 0.0972\n",
      "  Val Loss: 0.0972\n",
      "Epoch 88/250\n",
      "  Train Loss: 0.0971\n",
      "  Val Loss: 0.0969\n",
      "Epoch 89/250\n",
      "  Train Loss: 0.0970\n",
      "  Val Loss: 0.0968\n",
      "Epoch 90/250\n",
      "  Train Loss: 0.0968\n",
      "  Val Loss: 0.0966\n",
      "Epoch 91/250\n",
      "  Train Loss: 0.0966\n",
      "  Val Loss: 0.0964\n",
      "Epoch 92/250\n",
      "  Train Loss: 0.0964\n",
      "  Val Loss: 0.0962\n",
      "Epoch 93/250\n",
      "  Train Loss: 0.0962\n",
      "  Val Loss: 0.0960\n",
      "Epoch 94/250\n",
      "  Train Loss: 0.0960\n",
      "  Val Loss: 0.0959\n",
      "Epoch 95/250\n",
      "  Train Loss: 0.0958\n",
      "  Val Loss: 0.0956\n",
      "Epoch 96/250\n",
      "  Train Loss: 0.0956\n",
      "  Val Loss: 0.0954\n",
      "Epoch 97/250\n",
      "  Train Loss: 0.0954\n",
      "  Val Loss: 0.0952\n",
      "Epoch 98/250\n",
      "  Train Loss: 0.0952\n",
      "  Val Loss: 0.0950\n",
      "Epoch 99/250\n",
      "  Train Loss: 0.0950\n",
      "  Val Loss: 0.0948\n",
      "Epoch 100/250\n",
      "  Train Loss: 0.0947\n",
      "  Val Loss: 0.0945\n",
      "Epoch 101/250\n",
      "  Train Loss: 0.0945\n",
      "  Val Loss: 0.0942\n",
      "Epoch 102/250\n",
      "  Train Loss: 0.0942\n",
      "  Val Loss: 0.0939\n",
      "Epoch 103/250\n",
      "  Train Loss: 0.0939\n",
      "  Val Loss: 0.0936\n",
      "Epoch 104/250\n",
      "  Train Loss: 0.0936\n",
      "  Val Loss: 0.0933\n",
      "Epoch 105/250\n",
      "  Train Loss: 0.0933\n",
      "  Val Loss: 0.0929\n",
      "Epoch 106/250\n",
      "  Train Loss: 0.0929\n",
      "  Val Loss: 0.0925\n",
      "Epoch 107/250\n",
      "  Train Loss: 0.0925\n",
      "  Val Loss: 0.0921\n",
      "Epoch 108/250\n",
      "  Train Loss: 0.0921\n",
      "  Val Loss: 0.0917\n",
      "Epoch 109/250\n",
      "  Train Loss: 0.0917\n",
      "  Val Loss: 0.0913\n",
      "Epoch 110/250\n",
      "  Train Loss: 0.0913\n",
      "  Val Loss: 0.0910\n",
      "Epoch 111/250\n",
      "  Train Loss: 0.0910\n",
      "  Val Loss: 0.0906\n",
      "Epoch 112/250\n",
      "  Train Loss: 0.0905\n",
      "  Val Loss: 0.0902\n",
      "Epoch 113/250\n",
      "  Train Loss: 0.0902\n",
      "  Val Loss: 0.0898\n",
      "Epoch 114/250\n",
      "  Train Loss: 0.0898\n",
      "  Val Loss: 0.0895\n",
      "Epoch 115/250\n",
      "  Train Loss: 0.0895\n",
      "  Val Loss: 0.0891\n",
      "Epoch 116/250\n",
      "  Train Loss: 0.0891\n",
      "  Val Loss: 0.0887\n",
      "Epoch 117/250\n",
      "  Train Loss: 0.0887\n",
      "  Val Loss: 0.0883\n",
      "Epoch 118/250\n",
      "  Train Loss: 0.0883\n",
      "  Val Loss: 0.0880\n",
      "Epoch 119/250\n",
      "  Train Loss: 0.0879\n",
      "  Val Loss: 0.0876\n",
      "Epoch 120/250\n",
      "  Train Loss: 0.0876\n",
      "  Val Loss: 0.0872\n",
      "Epoch 121/250\n",
      "  Train Loss: 0.0871\n",
      "  Val Loss: 0.0868\n",
      "Epoch 122/250\n",
      "  Train Loss: 0.0868\n",
      "  Val Loss: 0.0864\n",
      "Epoch 123/250\n",
      "  Train Loss: 0.0863\n",
      "  Val Loss: 0.0859\n",
      "Epoch 124/250\n",
      "  Train Loss: 0.0859\n",
      "  Val Loss: 0.0855\n",
      "Epoch 125/250\n",
      "  Train Loss: 0.0855\n",
      "  Val Loss: 0.0851\n",
      "Epoch 126/250\n",
      "  Train Loss: 0.0850\n",
      "  Val Loss: 0.0846\n",
      "Epoch 127/250\n",
      "  Train Loss: 0.0846\n",
      "  Val Loss: 0.0842\n",
      "Epoch 128/250\n",
      "  Train Loss: 0.0841\n",
      "  Val Loss: 0.0837\n",
      "Epoch 129/250\n",
      "  Train Loss: 0.0837\n",
      "  Val Loss: 0.0833\n",
      "Epoch 130/250\n",
      "  Train Loss: 0.0832\n",
      "  Val Loss: 0.0828\n",
      "Epoch 131/250\n",
      "  Train Loss: 0.0828\n",
      "  Val Loss: 0.0823\n",
      "Epoch 132/250\n",
      "  Train Loss: 0.0823\n",
      "  Val Loss: 0.0818\n",
      "Epoch 133/250\n",
      "  Train Loss: 0.0818\n",
      "  Val Loss: 0.0813\n",
      "Epoch 134/250\n",
      "  Train Loss: 0.0813\n",
      "  Val Loss: 0.0808\n",
      "Epoch 135/250\n",
      "  Train Loss: 0.0808\n",
      "  Val Loss: 0.0803\n",
      "Epoch 136/250\n",
      "  Train Loss: 0.0803\n",
      "  Val Loss: 0.0798\n",
      "Epoch 137/250\n",
      "  Train Loss: 0.0797\n",
      "  Val Loss: 0.0792\n",
      "Epoch 138/250\n",
      "  Train Loss: 0.0792\n",
      "  Val Loss: 0.0787\n",
      "Epoch 139/250\n",
      "  Train Loss: 0.0786\n",
      "  Val Loss: 0.0781\n",
      "Epoch 140/250\n",
      "  Train Loss: 0.0780\n",
      "  Val Loss: 0.0774\n",
      "Epoch 141/250\n",
      "  Train Loss: 0.0772\n",
      "  Val Loss: 0.0765\n",
      "Epoch 142/250\n",
      "  Train Loss: 0.0762\n",
      "  Val Loss: 0.0752\n",
      "Epoch 143/250\n",
      "  Train Loss: 0.0749\n",
      "  Val Loss: 0.0740\n",
      "Epoch 144/250\n",
      "  Train Loss: 0.0736\n",
      "  Val Loss: 0.0727\n",
      "Epoch 145/250\n",
      "  Train Loss: 0.0723\n",
      "  Val Loss: 0.0714\n",
      "Epoch 146/250\n",
      "  Train Loss: 0.0710\n",
      "  Val Loss: 0.0702\n",
      "Epoch 147/250\n",
      "  Train Loss: 0.0700\n",
      "  Val Loss: 0.0692\n",
      "Epoch 148/250\n",
      "  Train Loss: 0.0690\n",
      "  Val Loss: 0.0683\n",
      "Epoch 149/250\n",
      "  Train Loss: 0.0683\n",
      "  Val Loss: 0.0675\n",
      "Epoch 150/250\n",
      "  Train Loss: 0.0674\n",
      "  Val Loss: 0.0667\n",
      "Epoch 151/250\n",
      "  Train Loss: 0.0667\n",
      "  Val Loss: 0.0660\n",
      "Epoch 152/250\n",
      "  Train Loss: 0.0660\n",
      "  Val Loss: 0.0654\n",
      "Epoch 153/250\n",
      "  Train Loss: 0.0654\n",
      "  Val Loss: 0.0648\n",
      "Epoch 154/250\n",
      "  Train Loss: 0.0648\n",
      "  Val Loss: 0.0642\n",
      "Epoch 155/250\n",
      "  Train Loss: 0.0641\n",
      "  Val Loss: 0.0636\n",
      "Epoch 156/250\n",
      "  Train Loss: 0.0636\n",
      "  Val Loss: 0.0630\n",
      "Epoch 157/250\n",
      "  Train Loss: 0.0631\n",
      "  Val Loss: 0.0625\n",
      "Epoch 158/250\n",
      "  Train Loss: 0.0625\n",
      "  Val Loss: 0.0620\n",
      "Epoch 159/250\n",
      "  Train Loss: 0.0620\n",
      "  Val Loss: 0.0615\n",
      "Epoch 160/250\n",
      "  Train Loss: 0.0615\n",
      "  Val Loss: 0.0610\n",
      "Epoch 161/250\n",
      "  Train Loss: 0.0610\n",
      "  Val Loss: 0.0604\n",
      "Epoch 162/250\n",
      "  Train Loss: 0.0604\n",
      "  Val Loss: 0.0599\n",
      "Epoch 163/250\n",
      "  Train Loss: 0.0599\n",
      "  Val Loss: 0.0594\n",
      "Epoch 164/250\n",
      "  Train Loss: 0.0594\n",
      "  Val Loss: 0.0589\n",
      "Epoch 165/250\n",
      "  Train Loss: 0.0589\n",
      "  Val Loss: 0.0584\n",
      "Epoch 166/250\n",
      "  Train Loss: 0.0583\n",
      "  Val Loss: 0.0579\n",
      "Epoch 167/250\n",
      "  Train Loss: 0.0579\n",
      "  Val Loss: 0.0574\n",
      "Epoch 168/250\n",
      "  Train Loss: 0.0574\n",
      "  Val Loss: 0.0569\n",
      "Epoch 169/250\n",
      "  Train Loss: 0.0569\n",
      "  Val Loss: 0.0564\n",
      "Epoch 170/250\n",
      "  Train Loss: 0.0565\n",
      "  Val Loss: 0.0560\n",
      "Epoch 171/250\n",
      "  Train Loss: 0.0560\n",
      "  Val Loss: 0.0555\n",
      "Epoch 172/250\n",
      "  Train Loss: 0.0555\n",
      "  Val Loss: 0.0551\n",
      "Epoch 173/250\n",
      "  Train Loss: 0.0551\n",
      "  Val Loss: 0.0546\n",
      "Epoch 174/250\n",
      "  Train Loss: 0.0547\n",
      "  Val Loss: 0.0542\n",
      "Epoch 175/250\n",
      "  Train Loss: 0.0543\n",
      "  Val Loss: 0.0538\n",
      "Epoch 176/250\n",
      "  Train Loss: 0.0538\n",
      "  Val Loss: 0.0534\n",
      "Epoch 177/250\n",
      "  Train Loss: 0.0535\n",
      "  Val Loss: 0.0530\n",
      "Epoch 178/250\n",
      "  Train Loss: 0.0531\n",
      "  Val Loss: 0.0527\n",
      "Epoch 179/250\n",
      "  Train Loss: 0.0527\n",
      "  Val Loss: 0.0522\n",
      "Epoch 180/250\n",
      "  Train Loss: 0.0523\n",
      "  Val Loss: 0.0518\n",
      "Epoch 181/250\n",
      "  Train Loss: 0.0519\n",
      "  Val Loss: 0.0515\n",
      "Epoch 182/250\n",
      "  Train Loss: 0.0515\n",
      "  Val Loss: 0.0511\n",
      "Epoch 183/250\n",
      "  Train Loss: 0.0511\n",
      "  Val Loss: 0.0506\n",
      "Epoch 184/250\n",
      "  Train Loss: 0.0507\n",
      "  Val Loss: 0.0502\n",
      "Epoch 185/250\n",
      "  Train Loss: 0.0503\n",
      "  Val Loss: 0.0499\n",
      "Epoch 186/250\n",
      "  Train Loss: 0.0500\n",
      "  Val Loss: 0.0495\n",
      "Epoch 187/250\n",
      "  Train Loss: 0.0496\n",
      "  Val Loss: 0.0492\n",
      "Epoch 188/250\n",
      "  Train Loss: 0.0493\n",
      "  Val Loss: 0.0488\n",
      "Epoch 189/250\n",
      "  Train Loss: 0.0489\n",
      "  Val Loss: 0.0484\n",
      "Epoch 190/250\n",
      "  Train Loss: 0.0485\n",
      "  Val Loss: 0.0481\n",
      "Epoch 191/250\n",
      "  Train Loss: 0.0481\n",
      "  Val Loss: 0.0478\n",
      "Epoch 192/250\n",
      "  Train Loss: 0.0478\n",
      "  Val Loss: 0.0473\n",
      "Epoch 193/250\n",
      "  Train Loss: 0.0474\n",
      "  Val Loss: 0.0470\n",
      "Epoch 194/250\n",
      "  Train Loss: 0.0470\n",
      "  Val Loss: 0.0467\n",
      "Epoch 195/250\n",
      "  Train Loss: 0.0468\n",
      "  Val Loss: 0.0464\n",
      "Epoch 196/250\n",
      "  Train Loss: 0.0465\n",
      "  Val Loss: 0.0460\n",
      "Epoch 197/250\n",
      "  Train Loss: 0.0461\n",
      "  Val Loss: 0.0457\n",
      "Epoch 198/250\n",
      "  Train Loss: 0.0458\n",
      "  Val Loss: 0.0454\n",
      "Epoch 199/250\n",
      "  Train Loss: 0.0455\n",
      "  Val Loss: 0.0452\n",
      "Epoch 200/250\n",
      "  Train Loss: 0.0452\n",
      "  Val Loss: 0.0448\n",
      "Epoch 201/250\n",
      "  Train Loss: 0.0449\n",
      "  Val Loss: 0.0446\n",
      "Epoch 202/250\n",
      "  Train Loss: 0.0447\n",
      "  Val Loss: 0.0443\n",
      "Epoch 203/250\n",
      "  Train Loss: 0.0444\n",
      "  Val Loss: 0.0440\n",
      "Epoch 204/250\n",
      "  Train Loss: 0.0441\n",
      "  Val Loss: 0.0437\n",
      "Epoch 205/250\n",
      "  Train Loss: 0.0439\n",
      "  Val Loss: 0.0435\n",
      "Epoch 206/250\n",
      "  Train Loss: 0.0436\n",
      "  Val Loss: 0.0433\n",
      "Epoch 207/250\n",
      "  Train Loss: 0.0434\n",
      "  Val Loss: 0.0431\n",
      "Epoch 208/250\n",
      "  Train Loss: 0.0432\n",
      "  Val Loss: 0.0428\n",
      "Epoch 209/250\n",
      "  Train Loss: 0.0430\n",
      "  Val Loss: 0.0426\n",
      "Epoch 210/250\n",
      "  Train Loss: 0.0428\n",
      "  Val Loss: 0.0424\n",
      "Epoch 211/250\n",
      "  Train Loss: 0.0424\n",
      "  Val Loss: 0.0422\n",
      "Epoch 212/250\n",
      "  Train Loss: 0.0424\n",
      "  Val Loss: 0.0421\n",
      "Epoch 213/250\n",
      "  Train Loss: 0.0422\n",
      "  Val Loss: 0.0419\n",
      "Epoch 214/250\n",
      "  Train Loss: 0.0419\n",
      "  Val Loss: 0.0415\n",
      "Epoch 215/250\n",
      "  Train Loss: 0.0417\n",
      "  Val Loss: 0.0414\n",
      "Epoch 216/250\n",
      "  Train Loss: 0.0416\n",
      "  Val Loss: 0.0412\n",
      "Epoch 217/250\n",
      "  Train Loss: 0.0414\n",
      "  Val Loss: 0.0410\n",
      "Epoch 218/250\n",
      "  Train Loss: 0.0411\n",
      "  Val Loss: 0.0408\n",
      "Epoch 219/250\n",
      "  Train Loss: 0.0410\n",
      "  Val Loss: 0.0406\n",
      "Epoch 220/250\n",
      "  Train Loss: 0.0408\n",
      "  Val Loss: 0.0405\n",
      "Epoch 221/250\n",
      "  Train Loss: 0.0406\n",
      "  Val Loss: 0.0403\n",
      "Epoch 222/250\n",
      "  Train Loss: 0.0403\n",
      "  Val Loss: 0.0400\n",
      "Epoch 223/250\n",
      "  Train Loss: 0.0403\n",
      "  Val Loss: 0.0399\n",
      "Epoch 224/250\n",
      "  Train Loss: 0.0400\n",
      "  Val Loss: 0.0397\n",
      "Epoch 225/250\n",
      "  Train Loss: 0.0398\n",
      "  Val Loss: 0.0395\n",
      "Epoch 226/250\n",
      "  Train Loss: 0.0397\n",
      "  Val Loss: 0.0394\n",
      "Epoch 227/250\n",
      "  Train Loss: 0.0395\n",
      "  Val Loss: 0.0392\n",
      "Epoch 228/250\n",
      "  Train Loss: 0.0393\n",
      "  Val Loss: 0.0390\n",
      "Epoch 229/250\n",
      "  Train Loss: 0.0393\n",
      "  Val Loss: 0.0389\n",
      "Epoch 230/250\n",
      "  Train Loss: 0.0390\n",
      "  Val Loss: 0.0387\n",
      "Epoch 231/250\n",
      "  Train Loss: 0.0388\n",
      "  Val Loss: 0.0385\n",
      "Epoch 232/250\n",
      "  Train Loss: 0.0386\n",
      "  Val Loss: 0.0383\n",
      "Epoch 233/250\n",
      "  Train Loss: 0.0384\n",
      "  Val Loss: 0.0381\n",
      "Epoch 234/250\n",
      "  Train Loss: 0.0382\n",
      "  Val Loss: 0.0380\n",
      "Epoch 235/250\n",
      "  Train Loss: 0.0382\n",
      "  Val Loss: 0.0379\n",
      "Epoch 236/250\n",
      "  Train Loss: 0.0380\n",
      "  Val Loss: 0.0377\n",
      "Epoch 237/250\n",
      "  Train Loss: 0.0378\n",
      "  Val Loss: 0.0375\n",
      "Epoch 238/250\n",
      "  Train Loss: 0.0376\n",
      "  Val Loss: 0.0374\n",
      "Epoch 239/250\n",
      "  Train Loss: 0.0376\n",
      "  Val Loss: 0.0372\n",
      "Epoch 240/250\n",
      "  Train Loss: 0.0373\n",
      "  Val Loss: 0.0370\n",
      "Epoch 241/250\n",
      "  Train Loss: 0.0371\n",
      "  Val Loss: 0.0369\n",
      "Epoch 242/250\n",
      "  Train Loss: 0.0371\n",
      "  Val Loss: 0.0368\n",
      "Epoch 243/250\n",
      "  Train Loss: 0.0368\n",
      "  Val Loss: 0.0365\n",
      "Epoch 244/250\n",
      "  Train Loss: 0.0366\n",
      "  Val Loss: 0.0363\n",
      "Epoch 245/250\n",
      "  Train Loss: 0.0365\n",
      "  Val Loss: 0.0362\n",
      "Epoch 246/250\n",
      "  Train Loss: 0.0363\n",
      "  Val Loss: 0.0360\n",
      "Epoch 247/250\n",
      "  Train Loss: 0.0362\n",
      "  Val Loss: 0.0359\n",
      "Epoch 248/250\n",
      "  Train Loss: 0.0360\n",
      "  Val Loss: 0.0357\n",
      "Epoch 249/250\n",
      "  Train Loss: 0.0359\n",
      "  Val Loss: 0.0356\n",
      "Epoch 250/250\n",
      "  Train Loss: 0.0357\n",
      "  Val Loss: 0.0354\n",
      "Model is on: cpu\n",
      "Epoch 1/250\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0033\n",
      "Epoch 2/250\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0033\n",
      "Epoch 3/250\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 4/250\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 5/250\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0031\n",
      "Epoch 6/250\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0030\n",
      "Epoch 7/250\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0029\n",
      "Epoch 8/250\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0028\n",
      "Epoch 9/250\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0027\n",
      "Epoch 10/250\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0026\n",
      "Epoch 11/250\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0025\n",
      "Epoch 12/250\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0025\n",
      "Epoch 13/250\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0025\n",
      "Epoch 14/250\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0024\n",
      "Epoch 15/250\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0024\n",
      "Epoch 16/250\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0024\n",
      "Epoch 17/250\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0023\n",
      "Epoch 18/250\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0022\n",
      "Epoch 19/250\n",
      "  Train Loss: 0.0020\n",
      "  Val Loss: 0.0020\n",
      "Epoch 20/250\n",
      "  Train Loss: 0.0018\n",
      "  Val Loss: 0.0020\n",
      "Epoch 21/250\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0019\n",
      "Epoch 22/250\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0017\n",
      "Epoch 23/250\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0017\n",
      "Epoch 24/250\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0016\n",
      "Epoch 25/250\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0014\n",
      "Epoch 26/250\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0013\n",
      "Epoch 27/250\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0013\n",
      "Epoch 28/250\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0012\n",
      "Epoch 29/250\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0012\n",
      "Epoch 30/250\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0011\n",
      "Epoch 31/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0011\n",
      "Epoch 32/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0011\n",
      "Epoch 33/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0011\n",
      "Epoch 34/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0011\n",
      "Epoch 35/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 36/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 37/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 38/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 39/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 40/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 41/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 42/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 43/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 44/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 45/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 46/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 47/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 48/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 49/250\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0008\n",
      "Epoch 50/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 51/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 52/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 53/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0007\n",
      "Epoch 54/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 55/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0007\n",
      "Epoch 56/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 57/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 58/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 59/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 60/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0006\n",
      "Epoch 61/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0006\n",
      "Epoch 62/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 63/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 64/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 65/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 66/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 67/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 68/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 69/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 70/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 71/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 72/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 73/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 74/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 75/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0004\n",
      "Epoch 76/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 77/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 78/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 79/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 80/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 81/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 82/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 83/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 84/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 85/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 86/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 87/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 88/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 89/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 90/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0003\n",
      "Epoch 91/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0003\n",
      "Epoch 92/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0003\n",
      "Epoch 93/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 94/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 95/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 96/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 97/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 98/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 99/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 100/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 101/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 102/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 103/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 104/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 105/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 106/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 107/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 108/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 109/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 110/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 111/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 112/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 113/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 114/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 115/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 116/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 117/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 118/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 119/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 120/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 121/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 122/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 123/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 124/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 125/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: inf\n",
      "Epoch 126/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 127/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 128/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 129/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 130/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 131/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 132/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 133/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 134/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 135/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 136/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 137/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 138/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 139/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 140/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 141/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 142/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 143/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 144/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 145/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 146/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 147/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 148/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 149/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 150/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 151/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 152/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 153/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 154/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 155/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 156/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 157/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 158/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 159/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 160/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 161/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 162/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 163/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 164/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 165/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 166/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 167/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 168/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 169/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 170/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 171/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 172/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 173/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 174/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 175/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 176/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 177/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 178/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 179/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 180/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 181/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 182/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 183/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 184/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 185/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 186/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 187/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 188/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 189/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 190/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 191/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 192/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 193/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 194/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 195/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 196/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 197/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 198/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 199/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 200/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 201/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 202/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 203/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 204/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 205/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 206/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 207/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 208/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 209/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 210/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 211/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 212/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 213/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 214/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 215/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 216/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 217/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 218/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 219/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 220/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 221/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 222/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 223/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Stopping training.\n",
      "Model is on: cpu\n",
      "Epoch 1/250\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 2/250\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 3/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 4/250\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 5/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 6/250\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 7/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 8/250\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 9/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 10/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 11/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 12/250\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0003\n",
      "Epoch 13/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 14/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 15/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 16/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0003\n",
      "Epoch 17/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0002\n",
      "Epoch 18/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0002\n",
      "Epoch 19/250\n",
      "  Train Loss: 0.0003\n",
      "  Val Loss: 0.0002\n",
      "Epoch 20/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 21/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 22/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 23/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 24/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 25/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 26/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 27/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 28/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 29/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 30/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 31/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 32/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 33/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 34/250\n",
      "  Train Loss: 0.0002\n",
      "  Val Loss: 0.0002\n",
      "Epoch 35/250\n",
      "  Train Loss: 0.0001\n",
      "  Val Loss: nan\n",
      "Epoch 36/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 37/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 38/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 39/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 40/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 41/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 42/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 43/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 44/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 45/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 46/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 47/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 48/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 49/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 50/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 51/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 52/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 53/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 54/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 55/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 56/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 57/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 58/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 59/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 60/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 61/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 62/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 63/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 64/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 65/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 66/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 67/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 68/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 69/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 70/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 71/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 72/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 73/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 74/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 75/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 76/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 77/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 78/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 79/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 80/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 81/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 82/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 83/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 84/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n",
      "Epoch 85/250\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m generator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      9\u001b[0m X_cal, y_cal \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m tau_corrs_LAC, skills_LAC, conformities_LAC, models_LAC \u001b[38;5;241m=\u001b[39m \u001b[43mconduct_oracle_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLACConformityScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pairs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# tau_corrs_Naive, skills_Naive, conformities_Naive = conduct_oracle_experiment(NaiveConformityScore(), num_pairs_to_check, generator, X_cal, y_cal)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[40], line 36\u001b[0m, in \u001b[0;36mconduct_oracle_experiment\u001b[0;34m(conformity_score, num_instances_to_check, generator, X_cal, y_cal)\u001b[0m\n\u001b[1;32m     34\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is on: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpair_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# generate data from data generating process and check whether the learned non-conformity relation sorts them correctly\u001b[39;00m\n\u001b[1;32m     40\u001b[0m X_test \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate_instances(\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Research/torch_plnet/models/ranking_models.py:312\u001b[0m, in \u001b[0;36mDyadRankingModel._fit\u001b[0;34m(self, train_loader, val_loader, learning_rate, num_epochs, random_state, patience, delta, verbose)\u001b[0m\n\u001b[1;32m    308\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    310\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m--> 312\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    314\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/torch/optim/adam.py:382\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    380\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    383\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[1;32m    384\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_pairs_to_check = np.linspace(10,100,3).astype(int)\n",
    "X_train, y_train = make_classification(\n",
    "    n_samples=100, n_features=3, n_classes=3, n_informative=3, n_redundant=0, n_repeated=0, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and fit the generator\n",
    "generator = MultinomialSyntheticDataGenerator(random_state=42)\n",
    "generator.fit(X_train, y_train)\n",
    "X_cal, y_cal = generator.generate(n=100)\n",
    "tau_corrs_LAC, skills_LAC, conformities_LAC, models_LAC = conduct_oracle_experiment(LACConformityScore(), num_pairs_to_check, generator, X_cal, y_cal)\n",
    "# tau_corrs_Naive, skills_Naive, conformities_Naive = conduct_oracle_experiment(NaiveConformityScore(), num_pairs_to_check, generator, X_cal, y_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0989\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0988\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0987\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0986\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0985\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0984\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0983\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0983\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0983\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0983\n",
      "  Val Loss: 0.0982\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0982\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0982\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0982\n",
      "  Val Loss: 0.0981\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0981\n",
      "  Val Loss: 0.0981\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0981\n",
      "  Val Loss: 0.0980\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0980\n",
      "  Val Loss: 0.0980\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0980\n",
      "  Val Loss: 0.0979\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0979\n",
      "  Val Loss: 0.0979\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0979\n",
      "  Val Loss: 0.0979\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0979\n",
      "  Val Loss: 0.0978\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0978\n",
      "  Val Loss: 0.0978\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0978\n",
      "  Val Loss: 0.0977\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0977\n",
      "  Val Loss: 0.0977\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0977\n",
      "  Val Loss: 0.0976\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0976\n",
      "  Val Loss: 0.0975\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0975\n",
      "  Val Loss: 0.0975\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0975\n",
      "  Val Loss: 0.0974\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0974\n",
      "  Val Loss: 0.0974\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0974\n",
      "  Val Loss: 0.0973\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0973\n",
      "  Val Loss: 0.0972\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0972\n",
      "  Val Loss: 0.0972\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0095\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0094\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0094\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0093\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0093\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0092\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0092\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0091\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0091\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0090\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0090\n",
      "  Val Loss: 0.0090\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0090\n",
      "  Val Loss: 0.0089\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0089\n",
      "  Val Loss: 0.0089\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0089\n",
      "  Val Loss: 0.0088\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0088\n",
      "  Val Loss: 0.0088\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0088\n",
      "  Val Loss: 0.0087\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0087\n",
      "  Val Loss: 0.0086\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0086\n",
      "  Val Loss: 0.0086\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0086\n",
      "  Val Loss: 0.0085\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0085\n",
      "  Val Loss: 0.0084\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0084\n",
      "  Val Loss: 0.0084\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0084\n",
      "  Val Loss: 0.0083\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0083\n",
      "  Val Loss: 0.0082\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0082\n",
      "  Val Loss: 0.0082\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0082\n",
      "  Val Loss: 0.0081\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0081\n",
      "  Val Loss: 0.0080\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0081\n",
      "  Val Loss: 0.0080\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0080\n",
      "  Val Loss: 0.0079\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0079\n",
      "  Val Loss: 0.0079\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0079\n",
      "  Val Loss: 0.0078\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0078\n",
      "  Val Loss: 0.0077\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0077\n",
      "  Val Loss: 0.0077\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0077\n",
      "  Val Loss: 0.0076\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0076\n",
      "  Val Loss: 0.0075\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0076\n",
      "  Val Loss: 0.0075\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0075\n",
      "  Val Loss: 0.0074\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0074\n",
      "  Val Loss: 0.0074\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0074\n",
      "  Val Loss: 0.0073\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0073\n",
      "  Val Loss: 0.0073\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0073\n",
      "  Val Loss: 0.0072\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0033\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0032\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0031\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0030\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0029\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0028\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0028\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0028\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0028\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0027\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0027\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0026\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0026\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0025\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0025\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0024\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0023\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0023\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0022\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0021\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0020\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0020\n",
      "  Val Loss: 0.0019\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0019\n",
      "  Val Loss: 0.0019\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0019\n",
      "  Val Loss: 0.0018\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0018\n",
      "  Val Loss: 0.0017\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0017\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0016\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0016\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0015\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0015\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0015\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0016\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0015\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0014\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0013\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0012\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0011\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0011\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0010\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0008\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0007\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0010\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0008\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0006\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0004\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0004\n",
      "  Val Loss: 0.0004\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0993\n",
      "  Val Loss: 0.0993\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0993\n",
      "  Val Loss: 0.0993\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0993\n",
      "  Val Loss: 0.0992\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0992\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0992\n",
      "  Val Loss: 0.0991\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0991\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0991\n",
      "  Val Loss: 0.0990\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0990\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0990\n",
      "  Val Loss: 0.0989\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0989\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0989\n",
      "  Val Loss: 0.0988\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0988\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0988\n",
      "  Val Loss: 0.0987\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0987\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0987\n",
      "  Val Loss: 0.0986\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0986\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0986\n",
      "  Val Loss: 0.0985\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0985\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0985\n",
      "  Val Loss: 0.0984\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0984\n",
      "  Val Loss: 0.0984\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0096\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0096\n",
      "  Val Loss: 0.0095\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0095\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0095\n",
      "  Val Loss: 0.0094\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0094\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0094\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0093\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0094\n",
      "  Val Loss: 0.0093\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0093\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0093\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0092\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0093\n",
      "  Val Loss: 0.0092\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0092\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0092\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0092\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0091\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0092\n",
      "  Val Loss: 0.0091\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0091\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0091\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0090\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0091\n",
      "  Val Loss: 0.0090\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0090\n",
      "  Val Loss: 0.0090\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0090\n",
      "  Val Loss: 0.0090\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0090\n",
      "  Val Loss: 0.0089\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0089\n",
      "  Val Loss: 0.0089\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0089\n",
      "  Val Loss: 0.0089\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0089\n",
      "  Val Loss: 0.0088\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0088\n",
      "  Val Loss: 0.0088\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0088\n",
      "  Val Loss: 0.0087\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0087\n",
      "  Val Loss: 0.0087\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0087\n",
      "  Val Loss: 0.0086\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0086\n",
      "  Val Loss: 0.0086\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0086\n",
      "  Val Loss: 0.0085\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0085\n",
      "  Val Loss: 0.0085\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0085\n",
      "  Val Loss: 0.0084\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0084\n",
      "  Val Loss: 0.0083\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0033\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0033\n",
      "  Val Loss: 0.0032\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0032\n",
      "  Val Loss: 0.0032\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0031\n",
      "  Val Loss: 0.0031\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0031\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0030\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0030\n",
      "  Val Loss: 0.0029\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0029\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0029\n",
      "  Val Loss: 0.0028\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0028\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0028\n",
      "  Val Loss: 0.0027\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0027\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0027\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0027\n",
      "  Val Loss: 0.0026\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0026\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0026\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0026\n",
      "  Val Loss: 0.0025\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0025\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0025\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0025\n",
      "  Val Loss: 0.0024\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0024\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0024\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0024\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0024\n",
      "  Val Loss: 0.0023\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0023\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0023\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0023\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0023\n",
      "  Val Loss: 0.0022\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0022\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0022\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0022\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0022\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0021\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0022\n",
      "  Val Loss: 0.0021\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0021\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0021\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0021\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0021\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0021\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0020\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0021\n",
      "  Val Loss: 0.0020\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0020\n",
      "  Val Loss: 0.0020\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0017\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0017\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0017\n",
      "  Val Loss: 0.0016\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0016\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0016\n",
      "  Val Loss: 0.0015\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0015\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0015\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0015\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0015\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0015\n",
      "  Val Loss: 0.0014\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0014\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0014\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0014\n",
      "  Val Loss: 0.0013\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0013\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0013\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0013\n",
      "  Val Loss: 0.0012\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0012\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0012\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0011\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0012\n",
      "  Val Loss: 0.0011\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0011\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0011\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0011\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0010\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0010\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0011\n",
      "  Val Loss: 0.0010\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0009\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Model is on: cpu\n",
      "Epoch 1/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 2/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 3/50\n",
      "  Train Loss: 0.0010\n",
      "  Val Loss: 0.0010\n",
      "Epoch 4/50\n",
      "  Train Loss: 0.0009\n",
      "  Val Loss: 0.0009\n",
      "Epoch 5/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 6/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0008\n",
      "Epoch 7/50\n",
      "  Train Loss: 0.0008\n",
      "  Val Loss: 0.0007\n",
      "Epoch 8/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 9/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 10/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 11/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 12/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 13/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 14/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 15/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0007\n",
      "Epoch 16/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0006\n",
      "Epoch 17/50\n",
      "  Train Loss: 0.0007\n",
      "  Val Loss: 0.0006\n",
      "Epoch 18/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 19/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 20/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 21/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 22/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 23/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 24/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 25/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0006\n",
      "Epoch 26/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 27/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 28/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 29/50\n",
      "  Train Loss: 0.0006\n",
      "  Val Loss: 0.0005\n",
      "Epoch 30/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 31/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 32/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 33/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 34/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 35/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 36/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 37/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 38/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 39/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 40/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 41/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 42/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 43/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 44/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 45/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 46/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 47/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 48/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 49/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n",
      "Epoch 50/50\n",
      "  Train Loss: 0.0005\n",
      "  Val Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "tau_corrs_APS, skills_APC, conformities_APC, models_APC = conduct_oracle_experiment(APSConformityScore(), num_pairs_to_check, generator, X_cal, y_cal)\n",
    "tau_corrs_TopK, skills_TopK, conformities_TopK, models_TopK = conduct_oracle_experiment(TopKConformityScore(), num_pairs_to_check, generator, X_cal, y_cal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5,rc={'text.usetex' : True})\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('font', **{'family': 'serif'})\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 3)\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,3, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].set_title(\"LAC\")\n",
    "axes[0].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "axes[0].set_xlabel(r\"No. Pairs\")\n",
    "axes[0].set_ylim([-0.1,1])\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_LAC, ax = axes[0], marker=\"o\")\n",
    "axes[1].set_title(\"APS\")\n",
    "axes[1].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "axes[1].set_xlabel(r\"No. Pairs\")\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = axes[1], marker=\"o\")\n",
    "axes[2].set_title(\"TopK\")\n",
    "axes[2].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "axes[2].set_xlabel(r\"No. Pairs\")\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_TopK, ax = axes[2], marker=\"o\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"replicating.pdf\")\n",
    "# axes[1].set_title(\"APS\")\n",
    "# axes[1].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "# axes[1].set_xlabel(r\"No. Pairs\")\n",
    "# sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo. Pairs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_pairs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_corrs_LAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLAC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mnum_pairs_to_check, y\u001b[38;5;241m=\u001b[39mtau_corrs_APS, ax \u001b[38;5;241m=\u001b[39m ax, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPS\u001b[39m\u001b[38;5;124m\"\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mnum_pairs_to_check, y\u001b[38;5;241m=\u001b[39mtau_corrs_TopK, ax \u001b[38;5;241m=\u001b[39m ax, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopK\u001b[39m\u001b[38;5;124m\"\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/relational.py:485\u001b[0m, in \u001b[0;36mlineplot\u001b[0;34m(data, x, y, hue, size, style, units, weights, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlineplot\u001b[39m(\n\u001b[1;32m    472\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    473\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, hue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, units\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# Handle deprecation of ci parameter\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     errorbar \u001b[38;5;241m=\u001b[39m _deprecate_ci(errorbar, ci)\n\u001b[0;32m--> 485\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_LinePlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrorbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrorbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_kws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[1;32m    496\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/relational.py:216\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[0;34m(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    204\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    213\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.linewidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    214\u001b[0m     )\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrorbar \u001b[38;5;241m=\u001b[39m errorbar\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/seaborn/_core/data.py:265\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    260\u001b[0m             ids[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(val)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Construct a tidy plot DataFrame. This will convert a number of\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# types automatically, aligning on index in case of pandas objects\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# TODO Note: this fails when variable specs *only* have scalars!\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, names, ids\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/plnet/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5,rc={'text.usetex' : True})\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc('font', **{'family': 'serif'})\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 3)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"\")\n",
    "ax.set_ylabel(r\"Kendalls $\\tau$\")\n",
    "ax.set_xlabel(r\"No. Pairs\")\n",
    "ax.set_ylim([-0.2,1])\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_LAC, ax = ax, marker=\"o\",label=\"LAC\", legend=False)\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = ax, marker=\"o\",label=\"APS\", legend=False)\n",
    "sns.lineplot(x=num_pairs_to_check, y=tau_corrs_TopK, ax = ax, marker=\"o\", label=\"TopK\", legend=False)\n",
    "lgd = fig.legend(loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.08), frameon=False)\n",
    "\n",
    "fig.tight_layout() \n",
    "plt.savefig(\"replicating.pdf\",bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "# axes[1].set_title(\"APS\")\n",
    "# axes[1].set_ylabel(r\"Kendalls $\\tau$\")\n",
    "# axes[1].set_xlabel(r\"No. Pairs\")\n",
    "# sns.lineplot(x=num_pairs_to_check, y=tau_corrs_APS, ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
