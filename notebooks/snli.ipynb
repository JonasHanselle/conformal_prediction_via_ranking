{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/home/ra43rid/torch_plnet/venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /home/ra43rid/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import llm_blender\n",
    "blender = llm_blender.Blender()\n",
    "blender.loadranker(\"llm-blender/PairRM\", device=\"cuda\") # load PairRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load SNLI dataset\n",
    "ds = load_dataset(\"ag_news\").shuffle(seed=42)\n",
    "full_dataset = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 127600\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates:  31%|███       | 31/100 [06:21<14:09, 12.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m labels_cal \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m cal_dataset]\n\u001b[1;32m     25\u001b[0m labels_test \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m test_dataset]\n\u001b[0;32m---> 27\u001b[0m cal_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cal_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcal_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# take scores of true labels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m cal_scores \u001b[38;5;241m=\u001b[39m cal_scores[np\u001b[38;5;241m.\u001b[39marange(cal_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), labels_cal]\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mcompute_cal_scores\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ds]\n\u001b[1;32m     14\u001b[0m candidates_texts \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe category is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m classes]]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mblender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torch_plnet/venv/lib/python3.10/site-packages/llm_blender/blender/blender.py:225\u001b[0m, in \u001b[0;36mBlender.rank\u001b[0;34m(self, inputs, candidates, instructions, return_scores, batch_size, disable_tqdm, **rank_kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mranker_config\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mranker_config\u001b[38;5;241m.\u001b[39mranker_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpairranker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     preds \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    227\u001b[0m     batch_scores \u001b[38;5;241m=\u001b[39m get_scores_from_cmps(preds)\n",
      "File \u001b[0;32m~/torch_plnet/venv/lib/python3.10/site-packages/llm_blender/pair_ranker/ranker.py:761\u001b[0m, in \u001b[0;36mCrossCompareReranker._full_predict\u001b[0;34m(self, source_ids, source_attention_mask, candidate_ids, candidate_attention_mask, scores)\u001b[0m\n\u001b[1;32m    759\u001b[0m     left_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     right_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m _outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_cand_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_cand_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright_cand_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_cand_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    768\u001b[0m preds \u001b[38;5;241m=\u001b[39m _outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/torch_plnet/venv/lib/python3.10/site-packages/llm_blender/pair_ranker/ranker.py:456\u001b[0m, in \u001b[0;36mCrossCompareReranker._forward\u001b[0;34m(self, source_ids, source_attention_mask, cand1_ids, cand1_attention_mask, cand2_ids, cand2_attention_mask, cand1_scores, cand2_scores)\u001b[0m\n\u001b[1;32m    450\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model(\n\u001b[1;32m    451\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    452\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    453\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    454\u001b[0m )\n\u001b[1;32m    455\u001b[0m encs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 456\u001b[0m source_idxs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_prefix_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m source_encs \u001b[38;5;241m=\u001b[39m encs[source_idxs[\u001b[38;5;241m0\u001b[39m], source_idxs[\u001b[38;5;241m1\u001b[39m], :]\n\u001b[1;32m    458\u001b[0m cand1_idxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mcand1_prefix_id)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "classes = [\"World\", \"Sports\", \"Business\", \"Science and Technology\"]\n",
    "\n",
    "# Extract labels\n",
    "y = dataset['label']  # Keeping it in Hugging Face format\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "k_folds = 5  # Change as needed\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "def compute_cal_scores(ds):\n",
    "    inputs = [f\"{x['text']}\" for x in ds]\n",
    "    candidates_texts = [[f\"The category is {x}\" for x in classes]]*len(inputs)\n",
    "    return blender.rank(inputs, candidates_texts, return_scores=True, batch_size=2**8)\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (test_idx, cal_idx) in enumerate(skf.split(range(len(dataset)), y)):\n",
    "    print(f\"Fold: {fold}\\n\")\n",
    "    test_dataset = dataset.select(test_idx)  # Get training subset\n",
    "    cal_dataset = dataset.select(cal_idx)    # Get test subset\n",
    "\n",
    "    labels_cal = [x[\"label\"] for x in cal_dataset]\n",
    "    labels_test = [x[\"label\"] for x in test_dataset]\n",
    "\n",
    "    cal_scores = compute_cal_scores(cal_dataset)\n",
    "    # take scores of true labels\n",
    "    cal_scores = cal_scores[np.arange(cal_scores.shape[0]), labels_cal]\n",
    "    pred_scores = compute_cal_scores(test_dataset)\n",
    "\n",
    "\n",
    "    alphas = [0.02, 0.05, 0.1, 0.2]\n",
    "    for alpha in alphas:\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\"alpha =\\t\\t\\t {alpha}\")\n",
    "        n = len(cal_scores)\n",
    "        threshold = np.quantile(cal_scores.flatten(), np.ceil((n+1)*(alpha))/n, method=\"inverted_cdf\")\n",
    "        pred_sets = [np.where(row > threshold)[0].tolist() for row in pred_scores]\n",
    "        predictions = np.argmax(pred_scores, axis=1)\n",
    "        coverage = np.mean([labels_test[i] in pred_sets[i] for i in range(n)])\n",
    "        avg_set_size = np.mean([len(s) for s in pred_sets])\n",
    "        median_set_size = np.median([len(s) for s in pred_sets])\n",
    "        accuracy = accuracy_score(labels_test, predictions)\n",
    "        print(f\"coverage =\\t\\t {coverage}\")\n",
    "        print(f\"mean set size =\\t\\t {avg_set_size}\")\n",
    "        print(f\"median set size =\\t {median_set_size}\")\n",
    "        print(f\"accuracy =\\t {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm_blender.blender.blender.Blender at 0x7fe6d81f8160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
